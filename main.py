# -*- coding: utf-8 -*-
"""Semantic_Embedding_Based_Recommender_System (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mUOz095WxDWkUSzOVKp0DwTKATmkEB_P
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pandas numpy torch scikit-learn sentence-transformers kagglehub joblib

"""# Code To Train Model and Test Run"""

import pandas as pd
import numpy as np
import torch
import joblib
import re
import os
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, hamming_loss, f1_score
from sentence_transformers import SentenceTransformer, util
import kagglehub

# ------------------------- Step 1: Clean Text -------------------------
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# ------------------------- Step 2: Load Dataset -------------------------
print("📦 Downloading dataset from Kaggle...")
path = kagglehub.dataset_download("spsayakpaul/arxiv-paper-abstracts")
csv_path = os.path.join(path, "arxiv_data.csv")

df = pd.read_csv(csv_path)
df.dropna(subset=["titles", "summaries", "terms"], inplace=True)
df = df[["titles", "summaries", "terms"]]

# ------------------------- Step 3: Preprocess -------------------------
df["text"] = (df["titles"] + " " + df["summaries"]).apply(clean_text)
df["terms"] = df["terms"].apply(eval)

# ------------------------- Step 4: Label Binarization -------------------------
mlb = MultiLabelBinarizer()
y = mlb.fit_transform(df["terms"])

# ------------------------- Step 5: TF-IDF Vectorization -------------------------
vectorizer = TfidfVectorizer(max_features=3000, stop_words='english')
X = vectorizer.fit_transform(df["text"])

# ------------------------- Step 6: Train/Test Split -------------------------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ------------------------- Step 7: Train MLP Classifier -------------------------
model = MLPClassifier(hidden_layer_sizes=(512, 256), activation='relu', solver='adam', early_stopping=True)
model.fit(X_train, y_train)

# ------------------------- Step 8: Save Artifacts -------------------------
joblib.dump(model, "mlp_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")
joblib.dump(mlb, "mlb.pkl")

# ------------------------- Step 9: Sentence Embeddings -------------------------
print("✨ Generating sentence embeddings...")
embedder = SentenceTransformer('all-MiniLM-L6-v2')
texts = df["text"].tolist()
titles = df["titles"].tolist()
corpus_embeddings = embedder.encode(texts, convert_to_tensor=True)

torch.save(corpus_embeddings, "corpus_embeddings.pt")
joblib.dump(titles, "titles.pkl")

# ------------------------- Step 10: Evaluation -------------------------
def calculate_multilabel_accuracy(y_true, y_pred):
    exact_matches = np.all(y_true == y_pred, axis=1)
    return np.mean(exact_matches)

def evaluate_model(model, X_test, y_test, mlb, threshold=0.3):
    proba = model.predict_proba(X_test)
    y_pred_thresh = (np.array(proba) >= threshold).astype(int)

    exact_acc = calculate_multilabel_accuracy(y_test, y_pred_thresh)
    ham_loss = hamming_loss(y_test, y_pred_thresh)
    f1_micro = f1_score(y_test, y_pred_thresh, average='micro', zero_division=0)
    f1_macro = f1_score(y_test, y_pred_thresh, average='macro', zero_division=0)
    partial_matches = np.logical_and(y_test, y_pred_thresh).sum(axis=1) > 0
    partial_match_rate = np.mean(partial_matches)

    print("\nEvaluation Metrics:")
    print(f"Exact Match Accuracy : {exact_acc * 100:.2f}%")
    print(f"Partial Match Rate   : {partial_match_rate:.4f}")

    return exact_acc

evaluate_model(model, X_test, y_test, mlb)

# ------------------------- Step 11: Predict and Recommend -------------------------
def predict_and_recommend(user_input_text, threshold=0.3, top_k=5):
    model = joblib.load("mlp_model.pkl")
    vectorizer = joblib.load("vectorizer.pkl")
    mlb = joblib.load("mlb.pkl")
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    corpus_embeddings = torch.load("corpus_embeddings.pt")
    titles = joblib.load("titles.pkl")

    cleaned_input = clean_text(user_input_text)
    input_vector = vectorizer.transform([cleaned_input])

    proba = model.predict_proba(input_vector)
    y_pred_thresh = (np.array(proba) >= threshold).astype(int)

    if not y_pred_thresh.any():
        top_idx = np.argmax(proba, axis=1)
        y_pred_thresh[0, top_idx[0]] = 1

    predicted_labels = mlb.inverse_transform(y_pred_thresh)

    query_embedding = embedder.encode(cleaned_input, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)
    recommended_titles = [titles[idx] for idx in top_results.indices]

    return predicted_labels, recommended_titles

# ------------------------- Step 12: CLI Interface -------------------------
print("🎉 Welcome to PaperPal — your adorable AI research buddy! 🧠💫\n")
print("🐾 Just tell me about your paper and I’ll find the best matches for you!\n")

while True:
    print("Let's begin! (Type 'exit' to leave anytime)\n")

    user_title = input("What's your paper's **title**?\n👉 ")
    if user_title.strip().lower() == "exit":
        print("\n👋 Bye bye, scholar! Keep pushing boundaries and publishing greatness!\n")
        break

    print("📄 And what's the **abstract**? (Type 'END' on a new line to finish)\n👉")
    lines = []
    while True:
        line = input()
        if line.strip().upper() == "END":
            break
        lines.append(line)
    user_abstract = "\n".join(lines)

    combined_text = user_title + " " + user_abstract
    labels, recommendations = predict_and_recommend(combined_text)

    print("\n📚 Top 5 Recommended Papers:")
    for idx, rec in enumerate(recommendations, 1):
        print(f" {idx}. 📘 {rec}")

    print("\n✨━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━✨\n")

"""# Code To Run From Pretrained Files"""

import torch
import joblib
import numpy as np
from sentence_transformers import SentenceTransformer, util
from sklearn.metrics import f1_score, hamming_loss

def clean_text(text):
    import re
    text = text.lower()
    text = re.sub(r"[^a-z\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

def evaluate_with_saved(X, y, threshold=0.3):
    model = joblib.load("mlp_model.pkl")
    mlb = joblib.load("mlb.pkl")

    proba = model.predict_proba(X)
    y_pred_thresh = (np.array(proba) >= threshold).astype(int)

    exact_matches = np.all(y == y_pred_thresh, axis=1)
    exact_acc = np.mean(exact_matches)
    partial_matches = np.logical_and(y, y_pred_thresh).sum(axis=1) > 0
    partial_match_rate = np.mean(partial_matches)
    f1_micro = f1_score(y, y_pred_thresh, average='micro', zero_division=0)
    f1_macro = f1_score(y, y_pred_thresh, average='macro', zero_division=0)
    ham_loss = hamming_loss(y, y_pred_thresh)
    label_accuracy = (y == y_pred_thresh).mean()

    print(f"\n📊 Evaluation Metrics (from saved model):")
    print(f"Trained Accuracy : {label_accuracy * 100:.2f}%")
    print(f"Partial Match Rate        : {partial_match_rate * 100:.2f}%")

def predict_and_recommend_with_saved(user_input_text, threshold=0.3, top_k=5):
    model = joblib.load("mlp_model.pkl")
    vectorizer = joblib.load("vectorizer.pkl")
    mlb = joblib.load("mlb.pkl")
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    corpus_embeddings = torch.load("corpus_embeddings.pt")
    titles = joblib.load("titles.pkl")

    cleaned_input = clean_text(user_input_text)
    input_vector = vectorizer.transform([cleaned_input])

    proba = model.predict_proba(input_vector)
    y_pred_thresh = (np.array(proba) >= threshold).astype(int)

    if not y_pred_thresh.any():
        top_idx = np.argmax(proba, axis=1)
        y_pred_thresh[0, top_idx[0]] = 1

    predicted_labels = mlb.inverse_transform(y_pred_thresh)

    query_embedding = embedder.encode(cleaned_input, convert_to_tensor=True)
    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)
    recommended_titles = [titles[idx] for idx in top_results.indices]

    return predicted_labels, recommended_titles

# ------------------------- CLI Interface -------------------------
print("🎉 Welcome to PaperPal — your adorable AI research buddy!\n")
print("🐾 Just tell me about your paper and I’ll find the best matches for you!\n")

while True:
    print("Let's begin! (Type 'exit' to leave anytime)\n")

    user_title = input("What's your paper's **title**?\n👉 ")
    if user_title.strip().lower() == "exit":
        print("\n👋 Bye bye, scholar! Keep pushing boundaries and publishing greatness!\n")
        break

    print("Document And what's the **abstract**? (Type 'END' on a new line to finish)\n👉")
    lines = []
    while True:
        line = input()
        if line.strip().upper() == "END":
            break
        lines.append(line)
    user_abstract = "\n".join(lines)

    combined_text = user_title + " " + user_abstract
    labels, recommendations = predict_and_recommend(combined_text)

    print("\nTop 5 Recommended Papers:")
    for idx, rec in enumerate(recommendations, 1):
        print(f" {idx}. 📘 {rec}")

    print("\n✨━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━✨\n")

evaluate_with_saved(X_test, y_test)